{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytw32siz4vLh",
        "outputId": "6ea2710f-741a-471c-f777-a1b65c000487"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJa_Nf5os4W"
      },
      "source": [
        "glove_50_path = \"\"\n",
        "glove_300_path = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ6Fc9ElLXbz"
      },
      "source": [
        "## Loading Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGWGNuIIkb0e",
        "outputId": "79931c39-c8bf-4825-b59f-89272c71aa84"
      },
      "source": [
        "###### Extra Utility for resolving errors in loading Hindi Embeddings ####\n",
        "import progressbar\n",
        "import numpy as np\n",
        "\n",
        "def return_lines(path_to_txt):\n",
        "  f = open(path_to_txt, \"r\", encoding=\"utf-8\")\n",
        "  lines = []\n",
        "  i = 0\n",
        "  while i<500000:\n",
        "    try:\n",
        "      line = f.readline()\n",
        "      lines.append(line)\n",
        "      i+=1\n",
        "      if not line:\n",
        "        break\n",
        "    except:\n",
        "      pass \n",
        "  f.close()\n",
        "  return lines\n",
        "\n",
        "def emb_matrix_maker(path_to_txt):\n",
        "  lines = return_lines(path_to_txt)\n",
        "  hindi_glove_dim = {}\n",
        "  for i in progressbar.progressbar(range(len(lines))):\n",
        "    values = lines[i].split(\" \")\n",
        "    hindi_glove_dim[values[0]] = np.asarray(values[1:], \"float32\")\n",
        "  return hindi_glove_dim\n",
        "\n",
        "def loadvocab(dim_size):\n",
        "  path_to_vocab = \"\"\n",
        "  if dim_size == 50:\n",
        "    path_to_vocab = glove_50_path\n",
        "  if dim_size == 300:\n",
        "    path_to_vocab = glove_300_path\n",
        "\n",
        "  f = open(path_to_vocab, \"r\")\n",
        "  freq_dict = {}\n",
        "  lines = []\n",
        "  i = 0\n",
        "  while i<500000:\n",
        "    try:\n",
        "      line = f.readline()\n",
        "      i = i + 1\n",
        "      lines.append(line)\n",
        "      \n",
        "      if not line:\n",
        "        break\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "  for line in lines:\n",
        "      values = line.split(\" \")\n",
        "      freq_dict[values[0]] = int(values[1])\n",
        "\n",
        "  f.close()\n",
        "  return freq_dict\n",
        "\n",
        "\n",
        "\n",
        "hindi_glove_50 = emb_matrix_maker(glove_50_path)\n",
        "hindi_glove_300 = emb_matrix_maker(glove_300_path)\n",
        "vocab_50 = loadvocab(50)\n",
        "vocab_300 = loadvocab(300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (500000 of 500000) |################| Elapsed Time: 0:00:10 Time:  0:00:10\n",
            "100% (500000 of 500000) |################| Elapsed Time: 0:00:40 Time:  0:00:40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x26JAeZRRprQ"
      },
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def do_pairwise_PCA(l1, l2, embedding, num_components = 10):\n",
        "#     matrix = []\n",
        "#     pairs = []\n",
        "#     for i, element in enumerate(l1):\n",
        "#       pairs.append([l1[i], l2[i]])\n",
        "\n",
        "#     for a, b in pairs:\n",
        "#         center = (embedding[a] + embedding[b])/2\n",
        "#         matrix.append(embedding[a] - center)\n",
        "#         matrix.append(embedding[b] - center)\n",
        "#     matrix = np.array(matrix)\n",
        "#     pca = PCA(n_components = num_components)\n",
        "#     pca.fit(matrix)\n",
        "#     # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "#     return pca.components_[0]\n",
        "\n",
        "# pairwise_gendered_PCA_l1 = [\"पिता\", \"बाप\", \"देव\", \"बंदा\", \"नर\"]\n",
        "# pairwise_gendered_PCA_l2 = [\"माता\", \"मां\", \"देवी\", \"बंदी\", \"नारी\"]\n",
        "\n",
        "# d_gender_pairwise_300 = do_pairwise_PCA(pairwise_gendered_PCA_l1, pairwise_gendered_PCA_l2, hindi_glove_300)\n",
        "\n",
        "# v = d_gender_pairwise_300\n",
        "\n",
        "# for word_key in hindi_glove_300.keys():\n",
        "#   original_emb = hindi_glove_300[key]\n",
        "#   new_emb = debiaser_new(original_emb)\n",
        "#   hindi_glove_300[key] = new_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwO20f2SLa8i"
      },
      "source": [
        "## WEAT Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TYs_HKsj-nC"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "# returns s(w, A, B) for all w in W (passed as argument). Shape: n_words (in W) x 1\n",
        "def swAB(W, A, B):\n",
        "  #Calculate cosine-similarity between W and A, W and B\n",
        "  #print(\"W: \", W.shape, \" A: \", A.shape, \" B: \", B.shape)\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  #print('WA shape: ', WA.shape)\n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  #print('sWAB shape: ', WAmean.shape)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
        "\n",
        "def weat_effect_size(X, Y, A, B, embd, debiased_weat=False):\n",
        "  #Convert the set of words to matrix\n",
        "  if (debiased_weat==False):\n",
        "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  else:\n",
        "    Xmat = np.array([debiaser(w,embd) for w in X if w.lower() in embd])\n",
        "    Ymat = np.array([debiaser(w,embd) for w in Y if w.lower() in embd])\n",
        "    #print(\"comes d\")\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  # Find X U Y\n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      if debiased_weat == False:\n",
        "        XuYmat.append(embd[w.lower()])\n",
        "      else:\n",
        "        XuYmat.append(debiaser(w,embd))\n",
        "  XuYmat = np.array(XuYmat)\n",
        "\n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample, debiased_weat=False):\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  if (debiased_weat==False):\n",
        "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  else:\n",
        "    Xmat = np.array([debiaser(w,embd) for w in X if w.lower() in embd])\n",
        "    Ymat = np.array([debiaser(w,embd) for w in Y if w.lower() in embd])\n",
        "    #print(\"comes p\")\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  #print(permutations)\n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    #print(Yi)\n",
        "    if debiased_weat == False :\n",
        "      Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "      Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    else:\n",
        "      Ximat = np.array([debiaser(w, embd) for w in Xi if w.lower() in embd])\n",
        "      Yimat = np.array([debiaser(w, embd) for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  #print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n88x7KzLe7J"
      },
      "source": [
        "## Hindi WEAT and SEAT test WORD LISTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTaOJmH2kxqq"
      },
      "source": [
        "X = {}\n",
        "Y = {}\n",
        "A = {}\n",
        "B = {}\n",
        "\n",
        "for i in range(30):\n",
        "  X[i] = {}\n",
        "  Y[i] = {}\n",
        "  A[i] = {}\n",
        "  B[i] = {}\n",
        "\n",
        "X[1][\"list\"] = [\"आदित्य\", \"अर्जुन\", \"ध्रुव\", \"ईशान\", \"कृष्णा\", \"सिद्धार्थ\", \"वीर\", \"शुभम\"]\n",
        "Y[1][\"list\"] = [\"अनिका\", \"अदिति\", \"दिया\", \"श्रेया\", \"मीरा\", \"जानवी\", \"नैना\", \"अंजलि\"]\n",
        "A[1][\"list\"] = [\"कार्यपालक\", \"प्रबंधन\", \"पेशेवर\", \"निगम\", \"वेतन\", \"कार्यालय\", \"व्यापार\", \"व्यवसाय\"]\n",
        "B[1][\"list\"] = [\"निवास\", \"पति\", \"बच्चे\", \"परिवार\", \"पत्नी\", \"शादी\", \"विवाह\", \"रिश्तेदार\"]\n",
        "X[1][\"type\"] = Y[1][\"type\"] = \"names\"\n",
        "A[1][\"type\"] = Y[1][\"type\"] = \"common_nouns\"\n",
        "\n",
        "#X[2][\"list\"] = [\"गणित\", \"बीजगणित\", \"ज्यामिति\", \"कलन\", \"समीकरण\", \"गणना\", \"संख्या\", \"योग\"]\n",
        "X[2][\"list\"] = [\"गणित\", \"बीजगणित\", \"ज्यामिति\", \"कलन\", \"समीकरण\", \"गणना\", \"संख्या\", \"जोड़\"]\n",
        "#Y[2][\"list\"] = [\"कविता\", \"कला\", \"नृत्य\", \"साहित्य\", \"उपन्यास\", \"राग\", \"नाटक\", \"मूर्ति\"]\n",
        "Y[2][\"list\"] = [\"कविता\", \"कला\", \"नृत्य\", \"साहित्य\", \"उपन्यास\", \"सिम्फनी\", \"नाटक\", \"मूर्तिकला\"]\n",
        "#A[2][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "A[2][\"list\"] = [\"नर\", \"आदमी\", \"लड़का\", \"भाई\", \"वह\", \"वह\", \"उसका\", \"बेटा\"]\n",
        "#B[2][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "B[2][\"list\"] = [\"महिला\", \"महिला\", \"लड़की\", \"बहन\", \"वह\", \"उसकी\", \"उसकी\", \"बेटी\"]\n",
        "X[2][\"type\"] = Y[2][\"type\"] = \"common_nouns\"\n",
        "A[2][\"type\"] = Y[2][\"type\"] = \"common_nouns\"\n",
        "\n",
        "#X[3][\"list\"] = [\"विज्ञान\", \"प्रौद्योगिकी\", \"भौतिक\", \"रसायन\", \"प्रयोगशाला\", \"नियम\", \"प्रयोग\", \"खगोल\"]\n",
        "X[3][\"list\"] = [\"विज्ञान\", \"प्रौद्योगिकी\", \"भौतिकी\", \"रसायन\", \"आइंस्टीन\", \"नासा\", \"प्रयोग\", \"खगोल\"]\n",
        "#Y[3][\"list\"] = [\"कविता\", \"कला\", \"नाच\", \"नृत्य\", \"साहित्य\", \"उपन्यास\", \"राग\", \"नाटक\"]\n",
        "Y[3][\"list\"] = [\"कविता\", \"कला\", \"शेक्सपियर\", \"नृत्य\", \"साहित्य\", \"उपन्यास\", \"सिम्फनी\", \"नाटक\"]\n",
        "#A[3][\"list\"] = [\"भाई\", \"पिता\", \"चाचा\", \"दादा\", \"बेटा\", \"पुरुष\", \"पति\", \"आदमी\"]\n",
        "A[3][\"list\"] = [\"भाई\", \"पिता\", \"चाचा\", \"दादा\", \"पुत्र\", \"वह\", \"उसका\", \"वह\"]\n",
        "#B[3][\"list\"] = [\"बहन\", \"मां\", \"चाची\", \"दादी\", \"बेटी\", \"महिला\", \"पत्नी\", \"औरत\"]\n",
        "B[3][\"list\"] = [\"बहन\", \"माँ\", \"चाची\", \"दादी\", \"बेटी\", \"वह\", \"उसकी\", \"उसकी\"]\n",
        "X[3][\"type\"] = Y[3][\"type\"] = \"common_nouns\"\n",
        "A[3][\"type\"] = Y[3][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[4][\"list\"] = [\"आदित्य\", \"अर्जुन\", \"ध्रुव\", \"ईशान\", \"कृष्णा\", \"सिद्धार्थ\", \"वीर\", \"शुभम\"]\n",
        "Y[4][\"list\"] = [\"अनिका\", \"अदिति\", \"दिया\", \"श्रेया\", \"मीरा\", \"जानवी\", \"नैना\", \"अंजलि\"]\n",
        "A[4][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[4][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[4][\"type\"] = Y[4][\"type\"] = \"names\"\n",
        "A[4][\"type\"] = Y[4][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[5][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "Y[5][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "A[5][\"list\"] = [\"आदित्य\", \"अर्जुन\", \"ध्रुव\", \"ईशान\", \"कृष्णा\", \"सिद्धार्थ\", \"वीर\", \"शुभम\"]\n",
        "B[5][\"list\"] = [\"अनिका\", \"अदिति\", \"दिया\", \"श्रेया\", \"मीरा\", \"जानवी\", \"नैना\", \"अंजलि\"]\n",
        "X[5][\"type\"] = Y[5][\"type\"] = \"common_nouns\"\n",
        "A[5][\"type\"] = Y[5][\"type\"] = \"names\"\n",
        "\n",
        "X[6][\"list\"] = [\"क्रोधित\", \"श्रमिक\", \"ताकतवर\", \"निपुण\", \"वीर\", \"साहसी\", \"दिलेर\"]\n",
        "Y[6][\"list\"] = [\"सुन्दर\", \"शर्म\", \"आकर्षक\", \"मनमोहक\", \"मधुर\", \"घरेलू\", \"कमज़ोर\" ]\n",
        "A[6][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[6][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[6][\"type\"] = Y[6][\"type\"] = \"adjectives\"\n",
        "A[6][\"type\"] = Y[6][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[7][\"list\"] = [\"गया\", \"आया\", \"खेलता\", \"बैठा\", \"लेता\", \"रहता\", \"देता\", \"पढ़ता\"]\n",
        "Y[7][\"list\"] = [\"गई\", \"आई\", \"खेलती\", \"बैठी\", \"लेती\", \"रहती\", \"देती\", \"पढ़ती\"]\n",
        "A[7][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[7][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[7][\"type\"] = Y[7][\"type\"] = \"verbs\"\n",
        "A[7][\"type\"] = Y[7][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[8][\"list\"] = [\"अच्छा\", \"बुरा\", \"गंदा\", \"लंबा\", \"छोटा\", \"मीठा\", \"नीला\", \"बड़ा\", \"पहला\"]\n",
        "Y[8][\"list\"] = [\"अच्छी\", \"बुरी\", \"गंदी\", \"लंबी\", \"छोटी\", \"मोती\", \"नीली\", \"बड़ी\", \"पहली\"]\n",
        "A[8][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[8][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[8][\"type\"] = Y[8][\"type\"] = \"adjectives\"\n",
        "A[8][\"type\"] = Y[8][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[9][\"list\"] = [\"पजामा\", \"घडा\", \"कुर्ता\", \"फूल\", \"कपड़ा\", \"पहिया\", \"यंत्र\", \"पुतला\", \"ताला\"]\n",
        "Y[9][\"list\"] = [\"अलमारी\", \"चादर\", \"पोशाक\", \"बिजली\", \"बुद्धि\", \"तस्वीर\", \"घडी\", \"राखी\", \"कमीज\"]\n",
        "A[9][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[9][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[9][\"type\"] = Y[9][\"type\"] = \"common_nouns\"\n",
        "A[9][\"type\"] = Y[9][\"type\"] = \"common_nouns\"\n",
        "\n",
        "X[10][\"list\"] = [\"अध्यापक\", \"शिष्य\", \"विद्वान\", \"साधु\", \"कवि\", \"छात्र\", \"प्रधानाचार्य\", \"महोदय\"]\n",
        "Y[10][\"list\"] = [\"अध्यापिका\", \"शिष्या\", \"विदुषी\", \"साध्वी\", \"कवित्री\", \"छात्रा\", \"प्रधानाचार्या\", \"महोदया\"]\n",
        "A[10][\"list\"] = [\"पुरुष\", \"आदमी\", \"लड़का\", \"भाई\", \"पति\", \"चाचा\", \"मामा\", \"बेटा\"]\n",
        "B[10][\"list\"] = [\"महिला\", \"औरत\", \"लड़की\", \"बहन\", \"पत्नी\", \"चाची\", \"मामी\", \"बेटी\"]\n",
        "X[10][\"type\"] = Y[10][\"type\"] = \"common_nouns\"\n",
        "A[10][\"type\"] = Y[10][\"type\"] = \"common_nouns\"\n",
        "\n",
        "##### 11-12 Caste #####\n",
        "\n",
        "X[11][\"list\"] = [\"व्यापर\", \"जमींदार\", \"सुनार\", \"गुरु\", \"मुनीम\", \"चिकित्सक\", \"पंडित\"]\n",
        "Y[11][\"list\"] = [\"सफाई\", \"धोबी\", \"मल्लाह\", \"माली\", \"नाई\", \"मोची\", \"मछुआरा\"]\n",
        "A[11][\"list\"] = [\"ठाकुर\", \"ब्राह्मण\", \"राजपूत\", \"क्षत्रिय\", \"आर्य\", \"जाट\", \"बनिया\", \"कायस्थ\"]\n",
        "B[11][\"list\"] = [\"दलित\", \"शूद्र\", \"भंगी\", \"चमार\", \"वाल्मीकि\", \"हरिजन\", \"चूहड़ा\", \"जाटव\"]\n",
        "X[11][\"type\"] = Y[11][\"type\"] = \"common_nouns\"\n",
        "A[11][\"type\"] = Y[11][\"type\"] = \"common_nouns\"\n",
        "\n",
        "#X[12][\"list\"] = [\"अमीर\", \"ज्ञानी\", \"वीर\", \"ताकतवर\", \"सुन्दर\", \"उच्च\", \"साहसी\"] \n",
        "X[12][\"list\"] = [\"दुलार\", \"स्वतंत्रता\", \"स्वास्थ्य\", \"प्रेम\", \"शांति\", \"जयकार\", \"मित्र\", \"स्वर्ग\", \"वफादार\", \"आनंद\", \"हीरा\", \"कोमल\", \"ईमानदार\", \"भाग्यशाली\", \"इंद्रधनुष\", \"डिप्लोमा\", \"उपहार\", \"सम्मान\", \"चमत्कार\", \"सूर्योदय\", \"परिवार\", \"खुश\", \"हँसी\", \"स्वर्ग\", \"छुट्टी\"]\n",
        "#Y[12][\"list\"] = [\"नीच\", \"घृणा\", \"गन्दा\", \"कमजोर\", \"गरीब\", \"अज्ञानी\", \"निर्बल\"]\n",
        "Y[12][\"list\"] = [\"दुर्व्यवहार\", \"दुर्घटना\", \"गंदगी\", \"हत्या\", \"बीमारी\", \"दुर्घटना\", \"मृत्यु\", \"दुख\", \"जहर\", \"बदबू\", \"हमला\", \"आपदा\", \"घृणा\", \"प्रदूषित\", \"त्रासदी\", \"बम\", \"तलाक\", \"जेल\", \"गरीबी\", \"बदसूरत\", \"कैंसर\", \"बुराई\", \"मार\", \"उल्टी\"]\n",
        "A[12][\"list\"] = [\"ठाकुर\", \"ब्राह्मण\", \"राजपूत\", \"क्षत्रिय\", \"आर्य\", \"जाट\", \"बनिया\", \"कायस्थ\"]\n",
        "B[12][\"list\"] = [\"दलित\", \"शूद्र\", \"भंगी\", \"चमार\", \"वाल्मीकि\", \"हरिजन\", \"चूहड़ा\", \"जाटव\"]\n",
        "X[12][\"type\"] = Y[12][\"type\"] = \"adjectives\"\n",
        "A[12][\"type\"] = Y[12][\"type\"] = \"common_nouns\"\n",
        "\n",
        "#### 13-15 Religion ####\n",
        "\n",
        "#X[13][\"list\"] = [\"शिक्षित\", \"वीर\", \"उच्च\", \"साहसी\", \"शांत\", \"दयालु\", \"सफल\"] \n",
        "#Y[13][\"list\"] = [\"नीच\", \"घृणा\", \"अशिक्षित\", \"हिंसक\", \"क्रोधी\", \"निर्दयी\", \"अत्याचारी\"]\n",
        "X[13][\"list\"] = [\"दुलार\", \"स्वतंत्रता\", \"स्वास्थ्य\", \"प्रेम\", \"शांति\", \"जयकार\", \"मित्र\", \"स्वर्ग\", \"वफादार\", \"आनंद\", \"हीरा\", \"कोमल\", \"ईमानदार\", \"भाग्यशाली\", \"इंद्रधनुष\", \"डिप्लोमा\", \"उपहार\", \"सम्मान\", \"चमत्कार\", \"सूर्योदय\", \"परिवार\", \"खुश\", \"हँसी\", \"स्वर्ग\", \"छुट्टी\"]\n",
        "Y[13][\"list\"] = [\"दुर्व्यवहार\", \"दुर्घटना\", \"गंदगी\", \"हत्या\", \"बीमारी\", \"दुर्घटना\", \"मृत्यु\", \"दुख\", \"जहर\", \"बदबू\", \"हमला\", \"आपदा\", \"घृणा\", \"प्रदूषित\", \"त्रासदी\", \"बम\", \"तलाक\", \"जेल\", \"गरीबी\", \"बदसूरत\", \"कैंसर\", \"बुराई\", \"मार\", \"उल्टी\"]\n",
        "A[13][\"list\"] = [\"हिंदू\", \"भगवान\", \"गीता\", \"ब्राह्मण\", \"पंडित\", \"मंदिर\", \"राम\", \"व्रत\"]\n",
        "B[13][\"list\"] = [\"मुसलमान\", \"अल्लाह\", \"कुरान\", \"शिया\", \"सुन्नी\", \"मस्जिद\", \"मुहम्मद\", \"रोजा\"]\n",
        "X[13][\"type\"] = Y[13][\"type\"] = \"adjectives\"\n",
        "A[13][\"type\"] = Y[13][\"type\"] = \"common_nouns\"\n",
        "\n",
        "#X[14][\"list\"] = [\"शिक्षित\", \"वीर\", \"उच्च\", \"साहसी\", \"शांत\", \"दयालु\", \"सफल\"] \n",
        "#Y[14][\"list\"] = [\"नीच\", \"घृणा\", \"अशिक्षित\", \"हिंसक\", \"क्रोधी\", \"निर्दयी\", \"अत्याचारी\"]\n",
        "X[14][\"list\"] = [\"दुलार\", \"स्वतंत्रता\", \"स्वास्थ्य\", \"प्रेम\", \"शांति\", \"जयकार\", \"मित्र\", \"स्वर्ग\", \"वफादार\", \"आनंद\", \"हीरा\", \"कोमल\", \"ईमानदार\", \"भाग्यशाली\", \"इंद्रधनुष\", \"डिप्लोमा\", \"उपहार\", \"सम्मान\", \"चमत्कार\", \"सूर्योदय\", \"परिवार\", \"खुश\", \"हँसी\", \"स्वर्ग\", \"छुट्टी\"]\n",
        "Y[14][\"list\"] = [\"दुर्व्यवहार\", \"दुर्घटना\", \"गंदगी\", \"हत्या\", \"बीमारी\", \"दुर्घटना\", \"मृत्यु\", \"दुख\", \"जहर\", \"बदबू\", \"हमला\", \"आपदा\", \"घृणा\", \"प्रदूषित\", \"त्रासदी\", \"बम\", \"तलाक\", \"जेल\", \"गरीबी\", \"बदसूरत\", \"कैंसर\", \"बुराई\", \"मार\", \"उल्टी\"]\n",
        "A[14][\"list\"] = [\"शर्मा\", \"वर्मा\", \"अग्रवाल\", \"गुप्ता\", \"चौहान\", \"बंसल\", \"मित्तल\", \"सिंह\", \"चौधरी\"]\n",
        "B[14][\"list\"] = [\"यूसुफ\", \"मलिक\", \"खान\", \"अंसारी\", \"शेख़\", \"अब्दुल्ला\", \"अहमद\", \"पठान\", \"मिर्जा\"]\n",
        "X[14][\"type\"] = Y[14][\"type\"] = \"adjectives\"\n",
        "A[14][\"type\"] = Y[14][\"type\"] = \"names\"\n",
        "\n",
        "X[15][\"list\"] = [\"भगवान\", \"गीता\", \"ब्राह्मण\", \"पंडित\", \"मंदिर\", \"राम\", \"व्रत\"]\n",
        "Y[15][\"list\"] = [ \"अल्लाह\", \"कुरान\", \"शिया\", \"सुन्नी\", \"मस्जिद\", \"मुहम्मद\", \"रोजा\"]\n",
        "A[15][\"list\"] = [\"हिंदू\", \"हिंदुत्व\"]\n",
        "B[15][\"list\"] = [\"मुसलमान\", \"इसलाम\"]\n",
        "X[15][\"type\"] = Y[15][\"type\"] = \"common_nouns\"\n",
        "A[15][\"type\"] = Y[15][\"type\"] = \"common_nouns\"\n",
        "\n",
        "### 16-17 Extras ####\n",
        "\n",
        "X[16][\"list\"] = [\"अमीर\", \"ज्ञानी\", \"वीर\", \"ताकतवर\", \"सुन्दर\", \"उच्च\", \"साहसी\"]  \n",
        "Y[16][\"list\"] = [\"नीच\", \"घृणा\", \"गन्दा\", \"कमजोर\", \"गरीब\", \"अज्ञानी\", \"निर्बल\"]\n",
        "A[16][\"list\"] = [\"कार्यपालक\", \"प्रबंधन\", \"बैंकर\", \"व्यवसायी\", \"इंजीनियर\", \"डॉक्टर\", \"वकील\", \"वैज्ञानिक\"]\n",
        "B[16][\"list\"] = [\"सफाईवाला\", \"माली\", \"चालक\", \"किसान\", \"हलवाई\", \"कुम्हार\", \"ग्वाला\", \"चरवाहा\"]\n",
        "X[16][\"type\"] = Y[16][\"type\"] = \"adjectives\"\n",
        "A[16][\"type\"] = Y[16][\"type\"] = \"common_nouns\"\n",
        "\n",
        "# X[17][\"list\"] = [\"अमीर\", \"ज्ञानी\", \"वीर\", \"ताकतवर\", \"सुन्दर\", \"उच्च\", \"साहसी\"]  \n",
        "# Y[17][\"list\"] = [\"नीच\", \"घृणा\", \"गन्दा\", \"कमजोर\", \"गरीब\", \"अज्ञानी\", \"निर्बल\"]\n",
        "X[17][\"list\"] = [\"दुलार\", \"स्वतंत्रता\", \"स्वास्थ्य\", \"प्रेम\", \"शांति\", \"जयकार\", \"मित्र\", \"स्वर्ग\", \"वफादार\", \"आनंद\", \"हीरा\", \"कोमल\", \"ईमानदार\", \"भाग्यशाली\", \"इंद्रधनुष\", \"डिप्लोमा\", \"उपहार\", \"सम्मान\", \"चमत्कार\", \"सूर्योदय\", \"परिवार\", \"खुश\", \"हँसी\", \"स्वर्ग\", \"छुट्टी\"]\n",
        "Y[17][\"list\"] = [\"दुर्व्यवहार\", \"दुर्घटना\", \"गंदगी\", \"हत्या\", \"बीमारी\", \"दुर्घटना\", \"मृत्यु\", \"दुख\", \"जहर\", \"बदबू\", \"हमला\", \"आपदा\", \"घृणा\", \"प्रदूषित\", \"त्रासदी\", \"बम\", \"तलाक\", \"जेल\", \"गरीबी\", \"बदसूरत\", \"कैंसर\", \"बुराई\", \"मार\", \"उल्टी\"]\n",
        "A[17][\"list\"] = [\"बैंकर\", \"व्यवसायी\", \"इंजीनियर\", \"वकील\", \"वैज्ञानिक\", \"चालक\", \"अभिनेता\", \"मैनेजर\"]\n",
        "B[17][\"list\"] = [\"लोहार\", \"जलवाहक\", \"किसान\", \"ग्वाला\", \"चरवाहा\", \"कुम्हार\", \"जमींदार\", \"जुलाहा\"]\n",
        "X[17][\"type\"] = Y[17][\"type\"] = \"adjectives\"\n",
        "A[17][\"type\"] = Y[17][\"type\"] = \"common_nouns\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qj7K1AgLmAg"
      },
      "source": [
        "## WEAT tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWkwabArmj9o",
        "outputId": "bbf82c2a-0ac7-4644-c7d3-428d8b4e4079"
      },
      "source": [
        "#For 50 dimensional embeddings\n",
        "embd = hindi_glove_50\n",
        "\n",
        "print(\"GloVe 50 dim:\")\n",
        "\n",
        "for i in range(1,18,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender WEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste WEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion WEAT:\")\n",
        "  if i==16:\n",
        "    print(\"\\nExtra WEAT:\")\n",
        "  print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=False), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=False)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GloVe 50 dim:\n",
            "\n",
            "Gender WEAT:\n",
            "0.249 (0.349)\n",
            "1.281 (0.002)\n",
            "1.415 (0.003)\n",
            "1.552 (0.001)\n",
            "1.318 (0.003)\n",
            "1.669 (0.000)\n",
            "1.863 (0.000)\n",
            "1.609 (0.000)\n",
            "1.171 (0.012)\n",
            "1.845 (0.000)\n",
            "\n",
            "Caste WEAT:\n",
            "1.391 (0.004)\n",
            "1.612 (0.001)\n",
            "\n",
            "Religion WEAT:\n",
            "1.305 (0.007)\n",
            "1.428 (0.003)\n",
            "1.744 (0.000)\n",
            "\n",
            "Extra WEAT:\n",
            "0.955 (0.041)\n",
            "1.297 (0.006)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpoywEAYoohJ",
        "outputId": "fc93f5c1-4aa7-4136-959a-6690c1732a31"
      },
      "source": [
        "#For 300 dimensional embeddings\n",
        "embd = hindi_glove_300\n",
        "\n",
        "print(\"GloVe 300 dim:\")\n",
        "\n",
        "for i in range(1,18,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender WEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste WEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion WEAT:\")\n",
        "  if i==16:\n",
        "    print(\"\\nExtra WEAT:\")\n",
        "  print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=False), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=False)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe 300 dim:\n",
            "\n",
            "Gender WEAT:\n",
            "0.473 (0.177)\n",
            "0.943 (0.024)\n",
            "0.274 (0.305)\n",
            "1.625 (0.000)\n",
            "1.719 (0.000)\n",
            "1.215 (0.016)\n",
            "1.874 (0.000)\n",
            "1.698 (0.000)\n",
            "1.138 (0.006)\n",
            "1.922 (0.000)\n",
            "\n",
            "Caste WEAT:\n",
            "1.439 (0.003)\n",
            "0.756 (0.003)\n",
            "\n",
            "Religion WEAT:\n",
            "1.048 (0.000)\n",
            "0.930 (0.000)\n",
            "1.751 (0.000)\n",
            "\n",
            "Extra WEAT:\n",
            "1.136 (0.009)\n",
            "-0.080 (0.594)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWHHlgNhLpei"
      },
      "source": [
        "## SEAT Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKKasSY5o664"
      },
      "source": [
        "SEAT_sent_dict = {}\n",
        "\n",
        "SEAT_sent_dict[\"names\"] = [\"यह _ है\", \"वह _ है\", \"वहाँ _ है\", \"यहां _ है\", \"_ यहां है\", \"_ वहाँ है\", \"इस्का नाम _ है\", \"उसका नाम _ है\"]\n",
        "SEAT_sent_dict[\"common_nouns\"] = [\"यह _ है\", \"वह _ है\", \"वहाँ _ है\", \"यहां _ है\", \"_ यहां है\", \"_ वहाँ है\", \"वो _ है\", \"ये _ है\"]\n",
        "SEAT_sent_dict[\"verbs\"] = [\"यह _ है\", \"वह _ है\", \"वो _ है\", \"ये _ है\", \"वहाँ _ है\", \"यहां _ है\"]\n",
        "SEAT_sent_dict[\"adjectives\"] = [\"वह _ है\", 'यह _ है', \"वो _ है\", \"ये _ है\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G17SvB00pYXm"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "\n",
        "def roberta_sentence_encoder(sentence, model, tokenizer, device='cuda'):\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  combined_toks = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "  encoded_sent = tokenizer.convert_tokens_to_ids(combined_toks)\n",
        "  input_ids = []\n",
        "  input_ids.append(encoded_sent)\n",
        "  #print(input_ids)\n",
        "  input_ids = pad_sequences(input_ids, maxlen=8, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "  #print(input_ids)\n",
        "  input_ids = input_ids[0]\n",
        "  #print(list(input_ids))\n",
        "  input_ids_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "  att_mask =[int(i > 0) for i in input_ids]\n",
        "  #print(att_mask)\n",
        "  att_mask_tensor = torch.tensor(att_mask).unsqueeze(0).to(device)\n",
        "  with torch.no_grad():\n",
        "    embedded = model(input_ids = input_ids_tensor, attention_mask=att_mask_tensor)\n",
        "\n",
        "  #print(embedded)\n",
        "  return embedded['last_hidden_state'][0][0].detach().cpu().numpy()\n",
        "\n",
        "def elmo_encoder(sentence, model, debiased_weat=False):\n",
        "  tokens = sentence.split(\" \")\n",
        "  vecs = model.get_elmo_vectors([tokens], layers=\"all\")\n",
        "  tok_embs = vecs[0][0]\n",
        "  sent_emb = np.mean(tok_embs, axis=0)\n",
        "\n",
        "  if (debiased_weat == True):\n",
        "    each_debiased_emb = []\n",
        "    for emb in tok_embs:\n",
        "      each_debiased_emb.append(debiaser_new(emb))\n",
        "    each_debiased_emb = np.array(each_debiased_emb)\n",
        "    return list(np.mean(each_debiased_emb, axis = 0))\n",
        "\n",
        "  return list(sent_emb)\n",
        "\n",
        "def sentence_encoder(sentence, placed_word=None, word_embedding_type=\"word\", E=None, encoder=None, return_connected_sentence=False, debiased_weat=False):\n",
        "\n",
        "  tok_sentence = sentence.split(\" \")\n",
        "  actual_tok_sentence = []\n",
        "  for tok in tok_sentence:\n",
        "    if tok != \"_\":\n",
        "      actual_tok_sentence.append(tok)\n",
        "    else:\n",
        "      actual_tok_sentence.append(placed_word)\n",
        "\n",
        "  connected = (\" \").join(actual_tok_sentence)\n",
        "  if return_connected_sentence==True:\n",
        "    return connected\n",
        "  #print(connected)\n",
        "\n",
        "  if word_embedding_type == \"word\":\n",
        "    encoded = []\n",
        "    for word in actual_tok_sentence:\n",
        "      if word == placed_word and debiased_weat==False:\n",
        "        #print(\"RUNS\")\n",
        "        encoded.append(E[word])\n",
        "      elif word == placed_word and debiased_weat==True:\n",
        "        encoded.append(debiaser(word, E))\n",
        "      else:\n",
        "        encoded.append(E[word])\n",
        "    #encoded = [E[word] for word in actual_tok_sentence]\n",
        "\n",
        "    return np.mean(encoded, axis=0)\n",
        "  \n",
        "  if word_embedding_type == \"elmo\":\n",
        "    return elmo_encoder(connected, model, debiased_weat=True)\n",
        "\n",
        "  if word_embedding_type == \"roberta\":\n",
        "    return roberta_sentence_encoder(connected, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "def sentence_iterator(word_type, word, SEAT_sent_dict, embd, return_connected_sentence=False, debiased_weat=False):\n",
        "  candidate_sentences = SEAT_sent_dict[word_type]\n",
        "  sentence_embeddings = []\n",
        "  for sentence in candidate_sentences:\n",
        "    #print(sentence)\n",
        "    if return_connected_sentence==True:\n",
        "      sentence_embeddings.append(sentence_encoder(sentence, placed_word=word, word_embedding_type=\"word\", E=embd, return_connected_sentence=True))\n",
        "    else:\n",
        "      sentence_embeddings.append(sentence_encoder(sentence, placed_word=word, word_embedding_type=\"word\", E=embd, debiased_weat=debiased_weat))\n",
        "  \n",
        "  #print(len(sentence_embeddings))\n",
        "  #print(\"\\n\")\n",
        "  return sentence_embeddings\n",
        "\n",
        "\n",
        "def seat_effect_size(X, Y, x_type, A, B, a_type, embd, debiased_weat=False):\n",
        "  X_full = []\n",
        "  Y_full = []\n",
        "  A_full = []\n",
        "  B_full = []\n",
        "  for w in X:\n",
        "    X_full = X_full + sentence_iterator(x_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in Y:\n",
        "    Y_full = Y_full + sentence_iterator(x_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in A:\n",
        "    A_full = A_full + sentence_iterator(a_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in B:\n",
        "    B_full = B_full + sentence_iterator(a_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "\n",
        "  #print(len(X_full), len(Y_full), len(A_full), len(B_full))\n",
        "  Xmat = np.array(X_full)\n",
        "  Ymat = np.array(Y_full)\n",
        "  Amat = np.array(A_full)\n",
        "  Bmat = np.array(B_full)\n",
        "  \n",
        "\n",
        "  XuYmat = np.array(X_full + Y_full)\n",
        "\n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "\n",
        "def seat_p_value(X, Y, x_type, A, B, a_type, embd, sample, debiased_weat=False):\n",
        "\n",
        "  X_full = []\n",
        "  Y_full = []\n",
        "  A_full = []\n",
        "  B_full = []\n",
        "  for w in X:\n",
        "    X_full = X_full + sentence_iterator(x_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in Y:\n",
        "    Y_full = Y_full + sentence_iterator(x_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in A:\n",
        "    A_full = A_full + sentence_iterator(a_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "  for w in B:\n",
        "    B_full = B_full + sentence_iterator(a_type, w, SEAT_sent_dict, embd, debiased_weat=debiased_weat)\n",
        "\n",
        "  Xmat = np.array(X_full)\n",
        "  Ymat = np.array(Y_full)\n",
        "  Amat = np.array(A_full)\n",
        "  Bmat = np.array(B_full)\n",
        "\n",
        "  size_of_permutation = min(len(X_full), len(Y_full))\n",
        "  X_Y_full = []\n",
        "  XpY = X + Y\n",
        "  for x in XpY:\n",
        "    X_Y_full = X_Y_full + sentence_iterator(x_type, x, SEAT_sent_dict, embd, return_connected_sentence=True, debiased_weat=debiased_weat)\n",
        "\n",
        "  #print(len(X_Y_full))\n",
        "  #print(X_Y_full)\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y_full, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    #print(len(Xi))\n",
        "    Yi = filterfalse(lambda e:e in Xi, X_Y_full)\n",
        "    #print(Xi)\n",
        "    #print(\"\\n\")\n",
        "    #print(Yi)\n",
        "    Ximat = np.array([sentence_encoder(sent, word_embedding_type=\"word\", E=embd, debiased_weat=debiased_weat) for sent in Xi])\n",
        "    Yimat = np.array([sentence_encoder(sent, word_embedding_type=\"word\", E=embd, debiased_weat=debiased_weat) for sent in Yi])\n",
        "    #print(Ximat)\n",
        "    #print(Yimat.shape)\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  #print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrS6tPF-LyFZ"
      },
      "source": [
        "## SEAT Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFsVWwuiqftE",
        "outputId": "28abc0ac-1deb-4a4b-e787-c02ccdc815bb"
      },
      "source": [
        "print(\"GloVe 50 dim:\")\n",
        "\n",
        "for i in range(1,18,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender SEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste SEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion SEAT:\")\n",
        "  if i==16:\n",
        "    print(\"\\nExtra SEAT:\")\n",
        "\n",
        "  print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], X[i][\"type\"], hindi_glove_50), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], X[i][\"type\"], hindi_glove_50, sample=1000)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GloVe 50 dim:\n",
            "\n",
            "Gender SEAT:\n",
            "0.039 (0.404)\n",
            "1.380 (0.000)\n",
            "1.435 (0.000)\n",
            "1.429 (0.000)\n",
            "1.166 (0.000)\n",
            "1.673 (0.000)\n",
            "1.880 (0.000)\n",
            "1.626 (0.000)\n",
            "1.224 (0.000)\n",
            "1.788 (0.000)\n",
            "\n",
            "Caste SEAT:\n",
            "1.267 (0.000)\n",
            "1.560 (0.000)\n",
            "\n",
            "Religion SEAT:\n",
            "1.169 (0.000)\n",
            "1.323 (0.000)\n",
            "1.556 (0.000)\n",
            "\n",
            "Extra SEAT:\n",
            "0.938 (0.000)\n",
            "1.355 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KYkh090rz96",
        "outputId": "81b7f9c3-796c-444e-b88e-a80146b098a2"
      },
      "source": [
        "print(\"GloVe 300 dim:\")\n",
        "\n",
        "for i in range(1,18,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender SEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste SEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion SEAT:\")\n",
        "  if i==16:\n",
        "    print(\"\\nExtra SEAT:\")\n",
        "\n",
        "  print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], X[i][\"type\"], hindi_glove_300), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], X[i][\"type\"], hindi_glove_300, sample=1000)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe 300 dim:\n",
            "\n",
            "Gender SEAT:\n",
            "0.153 (0.203)\n",
            "0.872 (0.000)\n",
            "0.183 (0.176)\n",
            "1.477 (0.000)\n",
            "1.523 (0.000)\n",
            "1.186 (0.000)\n",
            "1.839 (0.000)\n",
            "1.634 (0.000)\n",
            "1.121 (0.000)\n",
            "1.862 (0.000)\n",
            "\n",
            "Caste SEAT:\n",
            "1.257 (0.000)\n",
            "0.745 (0.000)\n",
            "\n",
            "Religion SEAT:\n",
            "1.043 (0.000)\n",
            "0.953 (0.000)\n",
            "1.693 (0.000)\n",
            "\n",
            "Extra SEAT:\n",
            "1.004 (0.000)\n",
            "-0.134 (0.883)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br4zBsW4L1Eb"
      },
      "source": [
        "## Preparing Hindi ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igq_UM1vr1Cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53764578-7159-443d-cef8-4d8121ec93ac"
      },
      "source": [
        "!wget https://www.cfilt.iitb.ac.in/~diptesh/embeddings/monolingual/contextual/hi.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-15 15:18:30--  https://www.cfilt.iitb.ac.in/~diptesh/embeddings/monolingual/contextual/hi.zip\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.134\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 372254783 (355M) [application/zip]\n",
            "Saving to: ‘hi.zip’\n",
            "\n",
            "hi.zip              100%[===================>] 355.01M  12.9MB/s    in 35s     \n",
            "\n",
            "2021-08-15 15:19:07 (10.0 MB/s) - ‘hi.zip’ saved [372254783/372254783]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au4cEaN6vSFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9b4528-ff56-4cc6-9d85-fee2a8b85c37"
      },
      "source": [
        "!unzip hi.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  hi.zip\n",
            "   creating: hi/\n",
            "   creating: hi/elmo/\n",
            "  inflating: hi/elmo/hi-d512-elmo.hdf5  \n",
            "  inflating: hi/elmo/hi-d512-vocab.txt  \n",
            "  inflating: hi/elmo/hi-d512-options.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpy08N1rvU1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0451752-1cdd-4d6e-bd90-3526e18dcf1b"
      },
      "source": [
        "!pip install --upgrade simple_elmo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simple_elmo\n",
            "  Downloading simple_elmo-0.8.0-py3-none-any.whl (45 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▏                        | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 20 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 30 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 40 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 45 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>1.8.1 in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (5.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->simple_elmo) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simple_elmo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simple_elmo) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->simple_elmo) (1.15.0)\n",
            "Installing collected packages: simple-elmo\n",
            "Successfully installed simple-elmo-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuxtLzGYwcFZ"
      },
      "source": [
        "!mv \"hi/elmo/hi-d512-options.json\" \"hi/elmo/options.json\"\n",
        "!mv \"hi/elmo/hi-d512-elmo.hdf5\" \"hi/elmo/elmo.hdf5\"\n",
        "!mv \"hi/elmo/hi-d512-vocab.txt\" \"hi/elmo/vocab.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1P9TBFpw8Of"
      },
      "source": [
        "# Change n_characters to 262 in options.json\n",
        "# Comment out print and warmup lines in elmo_helpers.py in /usr/local/lib/python3.7/dist-packages/simple_elmo/elmo_helpers.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "dD--_r_8va3P",
        "outputId": "fdfeab91-2233-493f-9f3a-1067188aeb78"
      },
      "source": [
        "from simple_elmo import ElmoModel\n",
        "model = ElmoModel()\n",
        "model.load(\"hi/elmo\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-15 15:23:27,236 : INFO : Loading model from hi/elmo...\n",
            "2021-08-15 15:23:27,237 : INFO : No model.hdf5 file found. Using hi/elmo/elmo.hdf5 as a model file.\n",
            "2021-08-15 15:23:27,242 : INFO : We will cache the vocabulary of 100 tokens.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The model is now loaded.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfSlFa32L5jX"
      },
      "source": [
        "## SEAT ELMo tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhLeAW5QwX9Q",
        "outputId": "81dd08b2-14e4-494d-8a03-70f230378b1f"
      },
      "source": [
        "print(\"Hindi ELMo\")\n",
        "\n",
        "for i in range(1,16,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender SEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste SEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion SEAT:\")\n",
        "\n",
        "  print('{0:.3f}'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hindi ELMo\n",
            "\n",
            "Gender SEAT:\n",
            "0.828\n",
            "-0.299\n",
            "0.505\n",
            "1.285\n",
            "1.333\n",
            "1.306\n",
            "1.711\n",
            "1.763\n",
            "1.682\n",
            "1.310\n",
            "\n",
            "Caste SEAT:\n",
            "1.139\n",
            "1.082\n",
            "\n",
            "Religion SEAT:\n",
            "1.217\n",
            "0.130\n",
            "0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osTPVbmc-oi8"
      },
      "source": [
        "## Preparing Hindi RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rHL60j0-upV",
        "outputId": "83735ea5-f0ea-4460-a976-129cdb31ac74"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkT6GO41-1pu"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import ElectraModel, ElectraTokenizer, ElectraConfig, AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\")\n",
        "model = AutoModel.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\")\n",
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh9AlB5G-16U",
        "outputId": "3062c344-7933-4b2c-88a0-74cb58e94538"
      },
      "source": [
        "print(\"Hindi RoBERTa\")\n",
        "\n",
        "for i in range(1,16,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender SEAT:\")\n",
        "  if i==11:\n",
        "    print(\"\\nCaste SEAT:\")\n",
        "  if i==13:\n",
        "    print(\"\\nReligion SEAT:\")\n",
        "\n",
        "  print('{0:.3f}'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hindi RoBERTa\n",
            "\n",
            "Gender SEAT:\n",
            "0.493\n",
            "-0.012\n",
            "0.329\n",
            "1.072\n",
            "0.428\n",
            "0.492\n",
            "0.474\n",
            "0.380\n",
            "-0.222\n",
            "1.311\n",
            "\n",
            "Caste SEAT:\n",
            "0.888\n",
            "-0.015\n",
            "\n",
            "Religion SEAT:\n",
            "0.417\n",
            "0.560\n",
            "0.228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SmXlDYUL8qx"
      },
      "source": [
        "## Word Lists for PCA debiasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIhkz34Qx2zd"
      },
      "source": [
        "listwise_gendered_PCA = [\"पिता\", \"बाप\", \"देव\", \"बंदा\", \"नर\", \"माता\", \"मां\", \"देवी\", \"बंदी\", \"नारी\"]\n",
        "\n",
        "pairwise_gendered_PCA_l1 = [\"पिता\", \"बाप\", \"देव\", \"बंदा\", \"नर\"]\n",
        "pairwise_gendered_PCA_l2 = [\"माता\", \"मां\", \"देवी\", \"बंदी\", \"नारी\"]\n",
        "\n",
        "pairwise_verbsan_PCA_l1 = [\"जानता\", \"बोलता\", \"देखता\", \"खाता\", \"चलता\", \"उड़ता\", \"जागता\"]\n",
        "pairwise_verbsan_PCA_l2 = [\"जानती\", \"बोलती\", \"देखती\", \"खाती\", \"चलती\", \"उड़ती\", \"जागती\"]\n",
        "\n",
        "pairwise_titlessan_PCA_l1 = [\"गायक\", \"लेखक\", \"प्रेमी\", \"बालक\", \"शिक्षक\"]\n",
        "pairwise_titlessan_PCA_l2 = [\"गायिका\", \"लेखिका\", \"प्रेमिका\", \"बालिका\", \"शिक्षिका\"]\n",
        "\n",
        "pairwise_adjsan_PCA_l1 = [\"दूसरा\", \"मोटा\", \"पतला\", \"गहरा\", \"महंगा\"]\n",
        "pairwise_adjsan_PCA_l2 = [\"दूसरी\", \"मोटी\", \"पतली\", \"गहरी\", \"महंगी\"]\n",
        "\n",
        "listwise_entsan_PCA = [\"रास्ता\", \"बस्ता\", \"चश्मा\", \"यान\", \"गीत\", \"केला\", \"सेब\", \"संतरा\", \"कुर्सी\", \"उंगली\", \"गाड़ी\", \"पुस्तक\", \"दवाई\", \"लकड़ी\", \"सब्जी\", \"रोटी\"]\n",
        "\n",
        "\n",
        "\n",
        "listwise_caste_PCA = [\"देसाई\", \"रॉय\", \"पाठक\", \"पंडित\", \"पुजारी\", \"कोल\", \"धनुक\", \"मज़हबी\", \"मुशहर\", \"घसिया\"]\n",
        "\n",
        "\n",
        "\n",
        "listwise_religion_lastnames_PCA = [\"रफीक\", \"मुस्तफा\", \"नासिर\", \"नवाज\", \"कासिम\", \"रेड्डी\", \"आचार्य\", \"पटेल\", \"आर्य\", \"कुमार\"]\n",
        "listwise_religion_PCA = [\"हज\", \"फतवा\", \"इस्लाम\", \"ईद\", \"भक्त\", \"स्वर्ग\", \"हिंदुत्व\", \"दिवाली\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtXIsuaGhOx3"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def do_pairwise_PCA(l1, l2, embedding, num_components = 10):\n",
        "    matrix = []\n",
        "    pairs = []\n",
        "    for i, element in enumerate(l1):\n",
        "      pairs.append([l1[i], l2[i]])\n",
        "\n",
        "    for a, b in pairs:\n",
        "        center = (embedding[a] + embedding[b])/2\n",
        "        matrix.append(embedding[a] - center)\n",
        "        matrix.append(embedding[b] - center)\n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components = num_components)\n",
        "    pca.fit(matrix)\n",
        "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "    return pca.components_[0]\n",
        "\n",
        "\n",
        "def do_listwise_PCA(list_words, embedding, num_components=10):\n",
        "  matrix = [embedding[word] for word in list_words]\n",
        "  matrix = np.array(matrix)\n",
        "  pca = PCA(n_components = num_components)\n",
        "  pca.fit(matrix)\n",
        "  return pca.components_[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agtd0fRKMCse"
      },
      "source": [
        "## Bias directions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlWQBcTphjSw"
      },
      "source": [
        "d_gender_listwise_50 = do_listwise_PCA(listwise_gendered_PCA, hindi_glove_50)\n",
        "d_gender_pairwise_50 = do_pairwise_PCA(pairwise_gendered_PCA_l1, pairwise_gendered_PCA_l2, hindi_glove_50)\n",
        "d_verbs_50 = do_pairwise_PCA(pairwise_verbsan_PCA_l1, pairwise_verbsan_PCA_l2, hindi_glove_50)\n",
        "d_adj_50 = do_pairwise_PCA(pairwise_adjsan_PCA_l1, pairwise_adjsan_PCA_l2, hindi_glove_50)\n",
        "d_titles_50 = do_pairwise_PCA(pairwise_titlessan_PCA_l1, pairwise_titlessan_PCA_l2, hindi_glove_50)\n",
        "d_ent_50 = do_listwise_PCA(listwise_entsan_PCA, hindi_glove_50)\n",
        "d_verbs_adj_50 = do_pairwise_PCA(pairwise_verbsan_PCA_l1 + pairwise_adjsan_PCA_l1, pairwise_verbsan_PCA_l2 + pairwise_adjsan_PCA_l2, hindi_glove_50)\n",
        "d_ent_titles_50 = do_listwise_PCA(pairwise_titlessan_PCA_l1 + pairwise_titlessan_PCA_l2 + listwise_entsan_PCA, hindi_glove_50)\n",
        "\n",
        "d_caste_50 = do_listwise_PCA(listwise_caste_PCA, hindi_glove_50)\n",
        "\n",
        "d_religion_50 = do_listwise_PCA(listwise_religion_lastnames_PCA, hindi_glove_50)\n",
        "d_religion_ent_50 = do_listwise_PCA(listwise_religion_PCA, hindi_glove_50, num_components=6)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_gender_listwise_300 = do_listwise_PCA(listwise_gendered_PCA, hindi_glove_300)\n",
        "d_gender_pairwise_300 = do_pairwise_PCA(pairwise_gendered_PCA_l1, pairwise_gendered_PCA_l2, hindi_glove_300)\n",
        "d_verbs_300 = do_pairwise_PCA(pairwise_verbsan_PCA_l1, pairwise_verbsan_PCA_l2, hindi_glove_300)\n",
        "d_adj_300 = do_pairwise_PCA(pairwise_adjsan_PCA_l1, pairwise_adjsan_PCA_l2, hindi_glove_300)\n",
        "d_titles_300 = do_pairwise_PCA(pairwise_titlessan_PCA_l1, pairwise_titlessan_PCA_l2, hindi_glove_300)\n",
        "d_ent_300 = do_listwise_PCA(listwise_entsan_PCA, hindi_glove_300)\n",
        "d_verbs_adj_300 = do_pairwise_PCA(pairwise_verbsan_PCA_l1 + pairwise_adjsan_PCA_l1, pairwise_verbsan_PCA_l2 + pairwise_adjsan_PCA_l2, hindi_glove_300)\n",
        "d_ent_titles_300 = do_listwise_PCA(pairwise_titlessan_PCA_l1 + pairwise_titlessan_PCA_l2 + listwise_entsan_PCA, hindi_glove_300)\n",
        "\n",
        "d_caste_300 = do_listwise_PCA(listwise_caste_PCA, hindi_glove_300)\n",
        "\n",
        "d_religion_300 = do_listwise_PCA(listwise_religion_lastnames_PCA, hindi_glove_300)\n",
        "d_religion_ent_300 = do_listwise_PCA(listwise_religion_PCA, hindi_glove_300, num_components=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm56K0pnka3c"
      },
      "source": [
        "## Gender Debiasing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18kQHpIMjV83"
      },
      "source": [
        "def debiaser(word, embd):\n",
        "  global v\n",
        "  v = v/np.linalg.norm(v)\n",
        "  u = np.array(embd[word])\n",
        "  debiased_word = u - np.dot(u,v)*v \n",
        "  return debiased_word\n",
        "\n",
        "\n",
        "def debiaser_new(emb):\n",
        "  global v\n",
        "  v = v/np.linalg.norm(v)\n",
        "  debiased_word = emb - np.dot(emb,v)*v \n",
        "  return debiased_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KldyHSsH05kp"
      },
      "source": [
        "### PCA Based debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XRGkGXuj-7h",
        "outputId": "1f6000df-7858-4deb-803b-8aae70d341db"
      },
      "source": [
        "print(\"### GENDER PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nGender debiased with listwise gendered words\")\n",
        "v = d_gender_listwise_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise gendered words\")\n",
        "v = d_gender_pairwise_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise verbs\")\n",
        "v = d_verbs_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise adjectives\")\n",
        "v = d_adj_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise titles\")\n",
        "v = d_titles_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with listwise entities\")\n",
        "v = d_ent_50\n",
        "run_deb(1,11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### GENDER PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Gender debiased with listwise gendered words\n",
            "0.628 (0.130)\n",
            "0.704 (0.089)\n",
            "1.140 (0.005)\n",
            "1.577 (0.000)\n",
            "1.372 (0.003)\n",
            "1.405 (0.004)\n",
            "1.750 (0.000)\n",
            "1.413 (0.002)\n",
            "0.939 (0.027)\n",
            "1.864 (0.000)\n",
            "\n",
            "Gender debiased with pairwise gendered words\n",
            "0.175 (0.379)\n",
            "0.738 (0.070)\n",
            "1.172 (0.007)\n",
            "1.486 (0.001)\n",
            "1.058 (0.016)\n",
            "1.020 (0.038)\n",
            "1.504 (0.000)\n",
            "1.434 (0.002)\n",
            "1.207 (0.006)\n",
            "1.890 (0.000)\n",
            "\n",
            "Gender debiased with pairwise verbs\n",
            "0.063 (0.447)\n",
            "1.297 (0.002)\n",
            "1.352 (0.001)\n",
            "1.602 (0.000)\n",
            "1.108 (0.006)\n",
            "1.665 (0.000)\n",
            "0.875 (0.046)\n",
            "-0.484 (0.819)\n",
            "0.146 (0.380)\n",
            "1.781 (0.000)\n",
            "\n",
            "Gender debiased with pairwise adjectives\n",
            "0.202 (0.370)\n",
            "1.475 (0.000)\n",
            "1.492 (0.000)\n",
            "1.523 (0.000)\n",
            "1.187 (0.007)\n",
            "1.540 (0.001)\n",
            "1.481 (0.001)\n",
            "0.238 (0.318)\n",
            "0.281 (0.281)\n",
            "1.831 (0.000)\n",
            "\n",
            "Gender debiased with pairwise titles\n",
            "-0.288 (0.690)\n",
            "1.333 (0.000)\n",
            "1.021 (0.018)\n",
            "0.672 (0.094)\n",
            "0.406 (0.214)\n",
            "1.456 (0.003)\n",
            "1.229 (0.002)\n",
            "1.593 (0.000)\n",
            "1.425 (0.000)\n",
            "-0.136 (0.595)\n",
            "\n",
            "Gender debiased with listwise entities\n",
            "0.187 (0.348)\n",
            "1.195 (0.011)\n",
            "1.416 (0.001)\n",
            "1.550 (0.001)\n",
            "1.400 (0.001)\n",
            "1.664 (0.000)\n",
            "1.852 (0.000)\n",
            "1.606 (0.000)\n",
            "1.127 (0.006)\n",
            "1.851 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY02hyy_lT_K",
        "outputId": "84e4116a-e674-4de3-dca8-e1844dc2b870"
      },
      "source": [
        "print(\"### GENDER PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nGender debiased with listwise gendered words\")\n",
        "v = d_gender_listwise_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise gendered words\")\n",
        "v = d_gender_pairwise_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise verbs\")\n",
        "v = d_verbs_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise adjectives\")\n",
        "v = d_adj_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise titles\")\n",
        "v = d_titles_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with listwise entities\")\n",
        "v = d_ent_300\n",
        "run_deb(1,11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### GENDER PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Gender debiased with listwise gendered words\n",
            "0.540 (0.179)\n",
            "0.724 (0.081)\n",
            "0.645 (0.093)\n",
            "1.626 (0.000)\n",
            "1.706 (0.000)\n",
            "0.913 (0.047)\n",
            "1.844 (0.000)\n",
            "1.628 (0.000)\n",
            "0.949 (0.019)\n",
            "1.926 (0.000)\n",
            "\n",
            "Gender debiased with pairwise gendered words\n",
            "0.360 (0.257)\n",
            "0.771 (0.064)\n",
            "0.001 (0.491)\n",
            "1.446 (0.001)\n",
            "1.492 (0.001)\n",
            "0.824 (0.082)\n",
            "1.790 (0.000)\n",
            "1.658 (0.000)\n",
            "0.998 (0.022)\n",
            "1.897 (0.000)\n",
            "\n",
            "Gender debiased with pairwise verbs\n",
            "0.368 (0.242)\n",
            "0.612 (0.120)\n",
            "1.139 (0.017)\n",
            "1.610 (0.000)\n",
            "1.578 (0.000)\n",
            "1.039 (0.030)\n",
            "1.293 (0.007)\n",
            "1.212 (0.001)\n",
            "0.600 (0.118)\n",
            "1.899 (0.000)\n",
            "\n",
            "Gender debiased with pairwise adjectives\n",
            "0.470 (0.201)\n",
            "1.355 (0.004)\n",
            "1.276 (0.007)\n",
            "1.678 (0.000)\n",
            "1.688 (0.000)\n",
            "1.034 (0.031)\n",
            "1.756 (0.000)\n",
            "0.833 (0.055)\n",
            "0.668 (0.095)\n",
            "1.928 (0.000)\n",
            "\n",
            "Gender debiased with pairwise titles\n",
            "0.089 (0.442)\n",
            "1.046 (0.020)\n",
            "0.696 (0.094)\n",
            "1.050 (0.021)\n",
            "0.970 (0.023)\n",
            "0.868 (0.067)\n",
            "1.531 (0.002)\n",
            "1.689 (0.000)\n",
            "1.108 (0.011)\n",
            "1.114 (0.011)\n",
            "\n",
            "Gender debiased with listwise entities\n",
            "0.495 (0.185)\n",
            "1.212 (0.008)\n",
            "1.169 (0.008)\n",
            "1.625 (0.000)\n",
            "1.732 (0.000)\n",
            "1.263 (0.008)\n",
            "1.875 (0.000)\n",
            "1.712 (0.000)\n",
            "1.172 (0.010)\n",
            "1.923 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Ado3L91Ajo"
      },
      "source": [
        "### Zhou Based debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghmtb8ApoWex",
        "outputId": "00b89a81-f9e3-40b0-b04f-61936bf76d1d"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_verbs_50)*d_verbs_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_adj_50)*d_adj_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_ent_50)*d_ent_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting entities direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_titles_50)*d_titles_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_ent_50)*d_ent_50 - np.dot(d_gender_pairwise_50, d_verbs_50)*d_verbs_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_verbs_adj_50)*d_verbs_adj_50 - np.dot(d_gender_pairwise_50, d_ent_titles_50)*d_ent_titles_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Gender debiased with Zhou (subtracting verbs direction from semantic gender):\n",
            "0.230 (0.356)\n",
            "0.981 (0.031)\n",
            "1.268 (0.004)\n",
            "1.479 (0.000)\n",
            "1.181 (0.006)\n",
            "1.270 (0.007)\n",
            "1.714 (0.000)\n",
            "1.594 (0.000)\n",
            "1.368 (0.001)\n",
            "1.884 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting adjectives direction from semantic gender):\n",
            "0.200 (0.331)\n",
            "0.804 (0.069)\n",
            "1.144 (0.007)\n",
            "1.517 (0.000)\n",
            "1.150 (0.011)\n",
            "1.230 (0.007)\n",
            "1.701 (0.000)\n",
            "1.645 (0.000)\n",
            "1.433 (0.000)\n",
            "1.883 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting entities direction from semantic gender):\n",
            "0.173 (0.365)\n",
            "0.765 (0.086)\n",
            "1.174 (0.001)\n",
            "1.488 (0.001)\n",
            "1.056 (0.016)\n",
            "1.019 (0.026)\n",
            "1.508 (0.002)\n",
            "1.437 (0.000)\n",
            "1.214 (0.003)\n",
            "1.889 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting titles direction from semantic gender):\n",
            "0.253 (0.327)\n",
            "1.033 (0.011)\n",
            "1.333 (0.001)\n",
            "1.501 (0.002)\n",
            "1.422 (0.002)\n",
            "1.479 (0.002)\n",
            "1.744 (0.000)\n",
            "1.540 (0.000)\n",
            "1.167 (0.007)\n",
            "1.889 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\n",
            "0.229 (0.326)\n",
            "0.998 (0.032)\n",
            "1.269 (0.003)\n",
            "1.480 (0.001)\n",
            "1.179 (0.007)\n",
            "1.270 (0.011)\n",
            "1.716 (0.000)\n",
            "1.595 (0.000)\n",
            "1.372 (0.002)\n",
            "1.884 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\n",
            "0.217 (0.337)\n",
            "1.139 (0.014)\n",
            "1.337 (0.002)\n",
            "1.479 (0.001)\n",
            "1.172 (0.005)\n",
            "1.232 (0.006)\n",
            "1.711 (0.000)\n",
            "1.644 (0.000)\n",
            "1.417 (0.001)\n",
            "1.881 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiLnU9vtsSZS",
        "outputId": "26806baa-f089-4df1-b234-8d34dddd8e61"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_verbs_300)*d_verbs_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_adj_300)*d_adj_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_ent_300)*d_ent_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting entities direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_titles_300)*d_titles_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_ent_300)*d_ent_300 - np.dot(d_gender_pairwise_300, d_verbs_300)*d_verbs_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_verbs_adj_300)*d_verbs_adj_300 - np.dot(d_gender_pairwise_300, d_ent_titles_300)*d_ent_titles_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Gender debiased with Zhou (subtracting verbs direction from semantic gender):\n",
            "0.410 (0.241)\n",
            "0.993 (0.021)\n",
            "0.266 (0.331)\n",
            "1.509 (0.002)\n",
            "1.593 (0.000)\n",
            "0.961 (0.034)\n",
            "1.849 (0.000)\n",
            "1.715 (0.000)\n",
            "1.144 (0.007)\n",
            "1.918 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting adjectives direction from semantic gender):\n",
            "0.367 (0.260)\n",
            "0.739 (0.066)\n",
            "-0.009 (0.501)\n",
            "1.458 (0.001)\n",
            "1.524 (0.000)\n",
            "0.917 (0.047)\n",
            "1.825 (0.000)\n",
            "1.728 (0.000)\n",
            "1.103 (0.010)\n",
            "1.902 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting entities direction from semantic gender):\n",
            "0.360 (0.237)\n",
            "0.702 (0.093)\n",
            "-0.032 (0.515)\n",
            "1.448 (0.001)\n",
            "1.496 (0.001)\n",
            "0.803 (0.089)\n",
            "1.789 (0.000)\n",
            "1.647 (0.000)\n",
            "0.983 (0.020)\n",
            "1.897 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting titles direction from semantic gender):\n",
            "0.467 (0.220)\n",
            "0.910 (0.032)\n",
            "0.499 (0.178)\n",
            "1.564 (0.000)\n",
            "1.686 (0.000)\n",
            "1.048 (0.027)\n",
            "1.846 (0.000)\n",
            "1.656 (0.000)\n",
            "1.062 (0.012)\n",
            "1.928 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\n",
            "0.410 (0.222)\n",
            "0.949 (0.029)\n",
            "0.239 (0.277)\n",
            "1.510 (0.001)\n",
            "1.595 (0.001)\n",
            "0.945 (0.051)\n",
            "1.849 (0.000)\n",
            "1.708 (0.000)\n",
            "1.133 (0.005)\n",
            "1.918 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\n",
            "0.403 (0.202)\n",
            "1.045 (0.016)\n",
            "0.413 (0.211)\n",
            "1.477 (0.001)\n",
            "1.591 (0.000)\n",
            "0.943 (0.048)\n",
            "1.849 (0.000)\n",
            "1.749 (0.000)\n",
            "1.181 (0.005)\n",
            "1.914 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EqBcylE1FW6"
      },
      "source": [
        "### Linear Projection debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzzIlyqLtEZ4",
        "outputId": "3020a214-54ad-4511-c181-5ce85dcf7186"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Gender Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"नारी\"]) - np.array(hindi_glove_50[\"नर\"])\n",
        "print(\"\\nGender debiased with Linear Direction (naari-nar):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = np.array(hindi_glove_50[\"माता\"]) - np.array(hindi_glove_50[\"पिता\"])\n",
        "print(\"\\nGender debiased with Linear Direction (maata-pita):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gender Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Gender debiased with Linear Direction (naari-nar):\n",
            "0.257 (0.335)\n",
            "0.596 (0.135)\n",
            "0.974 (0.021)\n",
            "1.480 (0.004)\n",
            "1.267 (0.000)\n",
            "1.478 (0.000)\n",
            "1.848 (0.000)\n",
            "1.536 (0.000)\n",
            "1.081 (0.005)\n",
            "1.854 (0.000)\n",
            "\n",
            "Gender debiased with Linear Direction (maata-pita):\n",
            "0.185 (0.355)\n",
            "1.388 (0.004)\n",
            "1.464 (0.002)\n",
            "1.614 (0.001)\n",
            "1.286 (0.003)\n",
            "1.534 (0.000)\n",
            "1.841 (0.000)\n",
            "1.710 (0.000)\n",
            "1.127 (0.014)\n",
            "1.777 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84iehr_Ezati",
        "outputId": "5698f5cc-a2af-4b22-b607-baf6ff65ee09"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Gender Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"नारी\"]) - np.array(hindi_glove_300[\"नर\"])\n",
        "print(\"\\nGender debiased with Linear Direction (naari-nar):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = np.array(hindi_glove_300[\"माता\"]) - np.array(hindi_glove_300[\"पिता\"])\n",
        "print(\"\\nGender debiased with Linear Direction (maata-pita):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gender Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Gender debiased with Linear Direction (naari-nar):\n",
            "0.489 (0.187)\n",
            "0.437 (0.203)\n",
            "0.497 (0.175)\n",
            "1.602 (0.002)\n",
            "1.676 (0.000)\n",
            "0.956 (0.056)\n",
            "1.867 (0.000)\n",
            "1.628 (0.000)\n",
            "1.009 (0.016)\n",
            "1.912 (0.000)\n",
            "\n",
            "Gender debiased with Linear Direction (maata-pita):\n",
            "0.502 (0.168)\n",
            "1.342 (0.002)\n",
            "1.186 (0.010)\n",
            "1.629 (0.000)\n",
            "1.689 (0.000)\n",
            "1.046 (0.029)\n",
            "1.861 (0.000)\n",
            "1.719 (0.000)\n",
            "1.136 (0.006)\n",
            "1.914 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtLonDLH3J8u"
      },
      "source": [
        "### PCA Based Debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os9NLr_lzpBO",
        "outputId": "047b4c62-0214-47b1-b8c8-dffa849cad2c"
      },
      "source": [
        "print(\"### GENDER PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nGender debiased with listwise gendered words\")\n",
        "v = d_gender_listwise_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise gendered words\")\n",
        "v = d_gender_pairwise_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise verbs\")\n",
        "v = d_verbs_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise adjectives\")\n",
        "v = d_adj_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise titles\")\n",
        "v = d_titles_50\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with listwise entities\")\n",
        "v = d_ent_50\n",
        "run_deb(1,11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### GENDER PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Gender debiased with listwise gendered words\n",
            "0.226 (0.107)\n",
            "0.993 (0.000)\n",
            "1.168 (0.000)\n",
            "1.552 (0.000)\n",
            "1.311 (0.000)\n",
            "1.391 (0.000)\n",
            "1.791 (0.000)\n",
            "1.454 (0.000)\n",
            "1.024 (0.000)\n",
            "1.780 (0.000)\n",
            "\n",
            "Gender debiased with pairwise gendered words\n",
            "-0.032 (0.557)\n",
            "0.892 (0.000)\n",
            "1.064 (0.000)\n",
            "1.456 (0.000)\n",
            "0.836 (0.000)\n",
            "1.036 (0.000)\n",
            "1.446 (0.000)\n",
            "1.398 (0.000)\n",
            "1.206 (0.000)\n",
            "1.604 (0.000)\n",
            "\n",
            "Gender debiased with pairwise verbs\n",
            "-0.092 (0.672)\n",
            "1.218 (0.000)\n",
            "1.279 (0.000)\n",
            "1.340 (0.000)\n",
            "0.997 (0.000)\n",
            "1.409 (0.000)\n",
            "0.656 (0.001)\n",
            "-0.294 (0.897)\n",
            "0.192 (0.110)\n",
            "1.645 (0.000)\n",
            "\n",
            "Gender debiased with pairwise adjectives\n",
            "-0.014 (0.528)\n",
            "1.555 (0.000)\n",
            "1.507 (0.000)\n",
            "1.370 (0.000)\n",
            "1.010 (0.000)\n",
            "1.495 (0.000)\n",
            "1.413 (0.000)\n",
            "0.218 (0.203)\n",
            "0.405 (0.004)\n",
            "1.735 (0.000)\n",
            "\n",
            "Gender debiased with pairwise titles\n",
            "-0.428 (0.993)\n",
            "1.159 (0.000)\n",
            "0.771 (0.000)\n",
            "0.748 (0.000)\n",
            "0.371 (0.021)\n",
            "1.187 (0.000)\n",
            "1.329 (0.000)\n",
            "1.589 (0.000)\n",
            "1.179 (0.000)\n",
            "0.017 (0.451)\n",
            "\n",
            "Gender debiased with listwise entities\n",
            "-0.009 (0.559)\n",
            "1.305 (0.000)\n",
            "1.433 (0.000)\n",
            "1.437 (0.000)\n",
            "1.207 (0.000)\n",
            "1.673 (0.000)\n",
            "1.877 (0.000)\n",
            "1.627 (0.000)\n",
            "1.174 (0.000)\n",
            "1.796 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCnniq-a3mnA",
        "outputId": "e93fdd3d-af56-4288-f45e-96413ef96858"
      },
      "source": [
        "print(\"### GENDER PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nGender debiased with listwise gendered words\")\n",
        "v = d_gender_listwise_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise gendered words\")\n",
        "v = d_gender_pairwise_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise verbs\")\n",
        "v = d_verbs_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise adjectives\")\n",
        "v = d_adj_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with pairwise titles\")\n",
        "v = d_titles_300\n",
        "run_deb(1,11)\n",
        "\n",
        "print(\"\\nGender debiased with listwise entities\")\n",
        "v = d_ent_300\n",
        "run_deb(1,11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### GENDER PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Gender debiased with listwise gendered words\n",
            "0.158 (0.186)\n",
            "0.901 (0.000)\n",
            "0.657 (0.000)\n",
            "1.502 (0.000)\n",
            "1.525 (0.000)\n",
            "0.969 (0.000)\n",
            "1.806 (0.000)\n",
            "1.592 (0.000)\n",
            "0.968 (0.000)\n",
            "1.878 (0.000)\n",
            "\n",
            "Gender debiased with pairwise gendered words\n",
            "0.056 (0.379)\n",
            "0.781 (0.000)\n",
            "0.071 (0.331)\n",
            "1.213 (0.000)\n",
            "1.174 (0.000)\n",
            "0.798 (0.001)\n",
            "1.670 (0.000)\n",
            "1.538 (0.000)\n",
            "0.992 (0.000)\n",
            "1.745 (0.000)\n",
            "\n",
            "Gender debiased with pairwise verbs\n",
            "0.031 (0.443)\n",
            "0.555 (0.002)\n",
            "0.804 (0.000)\n",
            "1.416 (0.000)\n",
            "1.377 (0.000)\n",
            "0.848 (0.001)\n",
            "1.023 (0.000)\n",
            "1.005 (0.000)\n",
            "0.570 (0.000)\n",
            "1.777 (0.000)\n",
            "\n",
            "Gender debiased with pairwise adjectives\n",
            "0.135 (0.226)\n",
            "1.320 (0.000)\n",
            "1.171 (0.000)\n",
            "1.505 (0.000)\n",
            "1.490 (0.000)\n",
            "1.005 (0.000)\n",
            "1.691 (0.000)\n",
            "0.740 (0.002)\n",
            "0.683 (0.000)\n",
            "1.858 (0.000)\n",
            "\n",
            "Gender debiased with pairwise titles\n",
            "-0.214 (0.866)\n",
            "0.858 (0.000)\n",
            "0.517 (0.001)\n",
            "1.079 (0.000)\n",
            "0.797 (0.000)\n",
            "0.722 (0.007)\n",
            "1.463 (0.000)\n",
            "1.643 (0.000)\n",
            "0.986 (0.000)\n",
            "0.898 (0.000)\n",
            "\n",
            "Gender debiased with listwise entities\n",
            "0.142 (0.214)\n",
            "1.168 (0.000)\n",
            "1.050 (0.000)\n",
            "1.472 (0.000)\n",
            "1.529 (0.000)\n",
            "1.199 (0.000)\n",
            "1.838 (0.000)\n",
            "1.640 (0.000)\n",
            "1.135 (0.000)\n",
            "1.859 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKYjJfFw6NUk"
      },
      "source": [
        "### Zhou Based Debiasing SEAT results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAaDXIFB6DAz",
        "outputId": "755ca522-4406-45cf-b9ee-880369659564"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_verbs_50)*d_verbs_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_adj_50)*d_adj_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_ent_50)*d_ent_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting entities direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_titles_50)*d_titles_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_ent_50)*d_ent_50 - np.dot(d_gender_pairwise_50, d_verbs_50)*d_verbs_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_50 - np.dot(d_gender_pairwise_50, d_verbs_adj_50)*d_verbs_adj_50 - np.dot(d_gender_pairwise_50, d_ent_titles_50)*d_ent_titles_50\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Gender debiased with Zhou (subtracting verbs direction from semantic gender):\n",
            "0.012 (0.466)\n",
            "1.111 (0.000)\n",
            "1.210 (0.000)\n",
            "1.397 (0.000)\n",
            "0.948 (0.000)\n",
            "1.274 (0.000)\n",
            "1.718 (0.000)\n",
            "1.598 (0.000)\n",
            "1.395 (0.000)\n",
            "1.730 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting adjectives direction from semantic gender):\n",
            "-0.010 (0.530)\n",
            "0.944 (0.000)\n",
            "1.057 (0.000)\n",
            "1.421 (0.000)\n",
            "0.943 (0.000)\n",
            "1.221 (0.000)\n",
            "1.689 (0.000)\n",
            "1.629 (0.000)\n",
            "1.429 (0.000)\n",
            "1.704 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting entities direction from semantic gender):\n",
            "-0.033 (0.577)\n",
            "0.906 (0.000)\n",
            "1.065 (0.000)\n",
            "1.456 (0.000)\n",
            "0.834 (0.000)\n",
            "1.034 (0.000)\n",
            "1.449 (0.000)\n",
            "1.400 (0.000)\n",
            "1.211 (0.000)\n",
            "1.604 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting titles direction from semantic gender):\n",
            "0.008 (0.481)\n",
            "1.195 (0.000)\n",
            "1.342 (0.000)\n",
            "1.451 (0.000)\n",
            "1.203 (0.000)\n",
            "1.475 (0.000)\n",
            "1.758 (0.000)\n",
            "1.559 (0.000)\n",
            "1.238 (0.000)\n",
            "1.807 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\n",
            "0.012 (0.489)\n",
            "1.120 (0.000)\n",
            "1.210 (0.000)\n",
            "1.396 (0.000)\n",
            "0.945 (0.000)\n",
            "1.273 (0.000)\n",
            "1.719 (0.000)\n",
            "1.598 (0.000)\n",
            "1.398 (0.000)\n",
            "1.730 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\n",
            "0.010 (0.476)\n",
            "1.196 (0.000)\n",
            "1.250 (0.000)\n",
            "1.369 (0.000)\n",
            "0.940 (0.000)\n",
            "1.247 (0.000)\n",
            "1.711 (0.000)\n",
            "1.635 (0.000)\n",
            "1.425 (0.000)\n",
            "1.729 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS96kMLs6n0I",
        "outputId": "51d62d4b-6422-420a-cd06-0d62d1a9eeee"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_verbs_300)*d_verbs_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_adj_300)*d_adj_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_ent_300)*d_ent_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting entities direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_titles_300)*d_titles_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_ent_300)*d_ent_300 - np.dot(d_gender_pairwise_300, d_verbs_300)*d_verbs_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = d_gender_pairwise_300 - np.dot(d_gender_pairwise_300, d_verbs_adj_300)*d_verbs_adj_300 - np.dot(d_gender_pairwise_300, d_ent_titles_300)*d_ent_titles_300\n",
        "print(\"\\nGender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Gender debiased with Zhou (subtracting verbs direction from semantic gender):\n",
            "0.102 (0.264)\n",
            "0.983 (0.000)\n",
            "0.278 (0.072)\n",
            "1.308 (0.000)\n",
            "1.301 (0.000)\n",
            "0.944 (0.000)\n",
            "1.796 (0.000)\n",
            "1.637 (0.000)\n",
            "1.137 (0.000)\n",
            "1.816 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting adjectives direction from semantic gender):\n",
            "0.061 (0.348)\n",
            "0.766 (0.000)\n",
            "0.061 (0.377)\n",
            "1.230 (0.000)\n",
            "1.214 (0.000)\n",
            "0.868 (0.001)\n",
            "1.725 (0.000)\n",
            "1.627 (0.000)\n",
            "1.088 (0.000)\n",
            "1.766 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting entities direction from semantic gender):\n",
            "0.057 (0.369)\n",
            "0.750 (0.000)\n",
            "0.047 (0.396)\n",
            "1.220 (0.000)\n",
            "1.181 (0.000)\n",
            "0.787 (0.001)\n",
            "1.673 (0.000)\n",
            "1.531 (0.000)\n",
            "0.983 (0.000)\n",
            "1.749 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting titles direction from semantic gender):\n",
            "0.139 (0.213)\n",
            "0.964 (0.000)\n",
            "0.472 (0.001)\n",
            "1.395 (0.000)\n",
            "1.449 (0.000)\n",
            "1.031 (0.000)\n",
            "1.793 (0.000)\n",
            "1.583 (0.000)\n",
            "1.059 (0.000)\n",
            "1.851 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs and adjectives direction from semantic gender):\n",
            "0.102 (0.295)\n",
            "0.961 (0.000)\n",
            "0.258 (0.061)\n",
            "1.312 (0.000)\n",
            "1.306 (0.000)\n",
            "0.935 (0.000)\n",
            "1.798 (0.000)\n",
            "1.632 (0.000)\n",
            "1.129 (0.000)\n",
            "1.818 (0.000)\n",
            "\n",
            "Gender debiased with Zhou (subtracting verbs+adj and ent_titles direction from semantic gender):\n",
            "0.093 (0.292)\n",
            "0.993 (0.000)\n",
            "0.384 (0.017)\n",
            "1.260 (0.000)\n",
            "1.282 (0.000)\n",
            "0.918 (0.000)\n",
            "1.781 (0.000)\n",
            "1.665 (0.000)\n",
            "1.161 (0.000)\n",
            "1.803 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXMaHbhq6-kF"
      },
      "source": [
        "### Linear Projection debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtPkRluM66Er",
        "outputId": "a99384fc-7a9a-4078-89a3-71cc67f0b23e"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Gender Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"नारी\"]) - np.array(hindi_glove_50[\"नर\"])\n",
        "print(\"\\nGender debiased with Linear Direction (naari-nar):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = np.array(hindi_glove_50[\"माता\"]) - np.array(hindi_glove_50[\"पिता\"])\n",
        "print(\"\\nGender debiased with Linear Direction (maata-pita):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gender Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Gender debiased with Linear Direction (naari-nar):\n",
            "0.047 (0.397)\n",
            "0.860 (0.000)\n",
            "1.098 (0.000)\n",
            "1.331 (0.000)\n",
            "1.109 (0.000)\n",
            "1.540 (0.000)\n",
            "1.868 (0.000)\n",
            "1.592 (0.000)\n",
            "1.177 (0.000)\n",
            "1.797 (0.000)\n",
            "\n",
            "Gender debiased with Linear Direction (maata-pita):\n",
            "-0.009 (0.527)\n",
            "1.372 (0.000)\n",
            "1.352 (0.000)\n",
            "1.418 (0.000)\n",
            "1.096 (0.000)\n",
            "1.396 (0.000)\n",
            "1.786 (0.000)\n",
            "1.635 (0.000)\n",
            "1.126 (0.000)\n",
            "1.667 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxPmdtdg7ZFz",
        "outputId": "312d4d57-ea2e-425e-ffc4-1394b3d54d08"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Gender Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"नारी\"]) - np.array(hindi_glove_300[\"नर\"])\n",
        "print(\"\\nGender debiased with Linear Direction (naari-nar):\")\n",
        "run_deb(1,11)\n",
        "\n",
        "v = np.array(hindi_glove_300[\"माता\"]) - np.array(hindi_glove_300[\"पिता\"])\n",
        "print(\"\\nGender debiased with Linear Direction (maata-pita):\")\n",
        "run_deb(1,11)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gender Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Gender debiased with Linear Direction (naari-nar):\n",
            "0.169 (0.147)\n",
            "0.637 (0.000)\n",
            "0.551 (0.000)\n",
            "1.453 (0.000)\n",
            "1.485 (0.000)\n",
            "0.981 (0.000)\n",
            "1.831 (0.000)\n",
            "1.581 (0.000)\n",
            "1.023 (0.000)\n",
            "1.848 (0.000)\n",
            "\n",
            "Gender debiased with Linear Direction (maata-pita):\n",
            "0.182 (0.158)\n",
            "1.195 (0.000)\n",
            "0.902 (0.000)\n",
            "1.435 (0.000)\n",
            "1.478 (0.000)\n",
            "0.932 (0.000)\n",
            "1.783 (0.000)\n",
            "1.588 (0.000)\n",
            "1.086 (0.000)\n",
            "1.814 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLhOrLgVasPj"
      },
      "source": [
        "### Debiasing ELMo first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLL5MerZYNGr"
      },
      "source": [
        "v = np.array(elmo_encoder(\"नर\", model)) - np.array(elmo_encoder(\"नारी\", model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Uu4Sr6YNPK",
        "outputId": "2ab1b7ca-b919-4af5-96f1-c25e367dde42"
      },
      "source": [
        "print(\"Hindi ELMo\")\n",
        "\n",
        "for i in range(1,11,1):\n",
        "  if i==1:\n",
        "    print(\"\\nGender SEAT:\")\n",
        "\n",
        "  print('{0:.3f}'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hindi ELMo\n",
            "\n",
            "Gender SEAT:\n",
            "0.749\n",
            "-0.391\n",
            "0.328\n",
            "1.113\n",
            "1.170\n",
            "1.317\n",
            "1.741\n",
            "1.751\n",
            "1.658\n",
            "1.274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXMjrzSfYNXF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzUQDaHdYNj0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIrDuu2_pna"
      },
      "source": [
        "## Caste Debiasing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piuHG8DdA7bx"
      },
      "source": [
        "### Linear Projection debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJKh7vTk8bnJ",
        "outputId": "2e9ee21b-4123-493c-d30c-827c4bc9d650"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Caste Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"घसिया\"]) - np.array(hindi_glove_50[\"देसाई\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-desai):\")\n",
        "run_deb(11,13)\n",
        "\n",
        "v = np.array(hindi_glove_50[\"घसिया\"]) - np.array(hindi_glove_50[\"पंडित\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-pandit):\")\n",
        "run_deb(11,13)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caste Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-desai):\n",
            "1.311 (0.011)\n",
            "1.593 (0.001)\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-pandit):\n",
            "0.914 (0.053)\n",
            "1.521 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll1xXgTtAhPh",
        "outputId": "efe215f0-c375-4e2d-ca29-8ff22a0034d6"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Caste Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"घसिया\"]) - np.array(hindi_glove_300[\"देसाई\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-desai):\")\n",
        "run_deb(11,13)\n",
        "\n",
        "v = np.array(hindi_glove_300[\"घसिया\"]) - np.array(hindi_glove_300[\"पंडित\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-pandit):\")\n",
        "run_deb(11,13)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caste Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-desai):\n",
            "1.339 (0.002)\n",
            "1.508 (0.001)\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-pandit):\n",
            "0.775 (0.086)\n",
            "1.312 (0.005)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxf95IcuCOED"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBSzWLV2CKry"
      },
      "source": [
        "### PCA Based debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlK6NNCA4OT",
        "outputId": "95d542da-67d5-4b46-932e-427149e0eac2"
      },
      "source": [
        "print(\"### CASTE PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nCaste debiased with listwise caste words\")\n",
        "v = d_caste_50\n",
        "run_deb(11,13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CASTE PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Caste debiased with listwise caste words\n",
            "1.227 (0.012)\n",
            "1.414 (0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-3lbBMVBQ9y",
        "outputId": "d60f6f10-2b8b-4daa-bcb5-b615fa325721"
      },
      "source": [
        "print(\"### CASTE PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nCaste debiased with listwise caste words\")\n",
        "v = d_caste_300\n",
        "run_deb(11,13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CASTE PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Caste debiased with listwise caste words\n",
            "1.212 (0.016)\n",
            "1.328 (0.003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC_ez27hDXJA"
      },
      "source": [
        "### Linear Projection debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLg9VcjJBaDr",
        "outputId": "afc9365b-caf4-46bb-fdda-64e77df84a4f"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Caste Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"घसिया\"]) - np.array(hindi_glove_50[\"देसाई\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-desai):\")\n",
        "run_deb(11,13)\n",
        "\n",
        "v = np.array(hindi_glove_50[\"घसिया\"]) - np.array(hindi_glove_50[\"पंडित\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-pandit):\")\n",
        "run_deb(11,13)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caste Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-desai):\n",
            "1.169 (0.000)\n",
            "1.494 (0.000)\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-pandit):\n",
            "0.678 (0.000)\n",
            "1.343 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPW8QnhVC30X",
        "outputId": "5cac5d63-c2b5-464d-9f91-6e8c8c07e220"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Caste Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"घसिया\"]) - np.array(hindi_glove_300[\"देसाई\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-desai):\")\n",
        "run_deb(11,13)\n",
        "\n",
        "v = np.array(hindi_glove_300[\"घसिया\"]) - np.array(hindi_glove_300[\"पंडित\"])\n",
        "print(\"\\nCaste debiased with Linear Direction (ghasiya-pandit):\")\n",
        "run_deb(11,13)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caste Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-desai):\n",
            "1.171 (0.000)\n",
            "1.359 (0.000)\n",
            "\n",
            "Caste debiased with Linear Direction (ghasiya-pandit):\n",
            "0.665 (0.000)\n",
            "1.184 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkWFHsKDcOq"
      },
      "source": [
        "### PCA based debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gpp2hphDDSo",
        "outputId": "bdc4a176-ce47-4c59-f364-5a948fd2a861"
      },
      "source": [
        "print(\"### CASTE PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nCaste debiased with listwise caste words\")\n",
        "v = d_caste_50\n",
        "run_deb(11,13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CASTE PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Caste debiased with listwise caste words\n",
            "0.991 (0.000)\n",
            "1.231 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgvqL_phDjI9",
        "outputId": "993f7e6c-ec30-4350-ad8f-69c990705ff1"
      },
      "source": [
        "print(\"### CASTE PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nCaste debiased with listwise caste words\")\n",
        "v = d_caste_300\n",
        "run_deb(11,13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CASTE PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Caste debiased with listwise caste words\n",
            "0.887 (0.000)\n",
            "1.181 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kxB41NTJCtt"
      },
      "source": [
        "## Religion Debiasing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Voj4UhPKp9R"
      },
      "source": [
        "### PCA Based debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtEJdOXnEBYK",
        "outputId": "e410a402-785c-4b7b-8568-3cacce22b0b4"
      },
      "source": [
        "print(\"### RELIGION PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion entities\")\n",
        "v = d_religion_ent_50\n",
        "run_deb(13,16)\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion lastnames\")\n",
        "v = d_religion_50\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### RELIGION PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Religion debiased with listwise religion entities\n",
            "0.821 (0.053)\n",
            "0.995 (0.039)\n",
            "1.340 (0.001)\n",
            "\n",
            "Religion debiased with listwise religion lastnames\n",
            "0.381 (0.253)\n",
            "0.568 (0.159)\n",
            "0.385 (0.262)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksGehticKLCX",
        "outputId": "b3eaad68-39e8-41a7-e414-b51916eb2b70"
      },
      "source": [
        "print(\"### RELIGION PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion entities\")\n",
        "v = d_religion_ent_300\n",
        "run_deb(13,16)\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion lastnames\")\n",
        "v = d_religion_300\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### RELIGION PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Religion debiased with listwise religion entities\n",
            "1.276 (0.006)\n",
            "1.551 (0.000)\n",
            "1.723 (0.000)\n",
            "\n",
            "Religion debiased with listwise religion lastnames\n",
            "0.909 (0.044)\n",
            "0.711 (0.098)\n",
            "1.544 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4szx5-KxNb"
      },
      "source": [
        "### Linear Projection based debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn63F_nvKcml",
        "outputId": "493680c1-75ee-490e-d94d-527bcd7f3f46"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Religion Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"आचार्य\"]) - np.array(hindi_glove_50[\"नासिर\"])\n",
        "print(\"\\nReligion debiased with Linear Direction (acharya-nasir):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Religion Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Religion debiased with Linear Direction (acharya-nasir):\n",
            "1.163 (0.023)\n",
            "1.601 (0.000)\n",
            "1.030 (0.029)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs8BuTvOMWdz",
        "outputId": "918987be-f9cb-4177-c470-9e96f5259773"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Religion Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"आचार्य\"]) - np.array(hindi_glove_300[\"नासिर\"])\n",
        "print(\"\\nReligion debiased with Linear Direction (acharya-nasir):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Religion Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Religion debiased with Linear Direction (acharya-nasir):\n",
            "1.276 (0.007)\n",
            "1.569 (0.000)\n",
            "1.612 (0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlq18LZeNDNZ"
      },
      "source": [
        "### Zhou Based Debiasing WEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U36p9eGSMxSi",
        "outputId": "db44ecfb-db9c-4c25-e31b-6392cb895b69"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_50, 1000, debiased_weat=True)))\n",
        "\n",
        "\n",
        "v = d_religion_50 - np.dot(d_religion_50, d_religion_ent_50)*d_religion_ent_50\n",
        "print(\"\\nReligion debiased with Zhou (subtracting entities direction):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Religion debiased with Zhou (subtracting entities direction):\n",
            "0.947 (0.047)\n",
            "1.174 (0.006)\n",
            "1.441 (0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTchdUohOHAN",
        "outputId": "a6d4cb28-63f9-4f4f-9516-59d4515dffa1"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(weat_effect_size(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, debiased_weat=True), weat_p_value(X[i][\"list\"], Y[i][\"list\"], A[i][\"list\"], B[i][\"list\"], hindi_glove_300, 1000, debiased_weat=True)))\n",
        "\n",
        "\n",
        "v = d_religion_300 - np.dot(d_religion_300, d_religion_ent_300)*d_religion_ent_300\n",
        "print(\"\\nReligion debiased with Zhou (subtracting entities direction):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Religion debiased with Zhou (subtracting entities direction):\n",
            "0.921 (0.057)\n",
            "0.707 (0.108)\n",
            "1.589 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqztu9wCORwb"
      },
      "source": [
        "### PCA Based debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVz01xdQONeT",
        "outputId": "c5a35eb5-69cf-4a49-8749-71e623a6729c"
      },
      "source": [
        "print(\"### RELIGION PCA DEBIASING RESULTS: 50 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion entities\")\n",
        "v = d_religion_ent_50\n",
        "run_deb(13,16)\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion lastnames\")\n",
        "v = d_religion_50\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### RELIGION PCA DEBIASING RESULTS: 50 DIM GLOVE ###\n",
            "\n",
            "Religion debiased with listwise religion entities\n",
            "0.794 (0.001)\n",
            "0.983 (0.000)\n",
            "1.161 (0.000)\n",
            "\n",
            "Religion debiased with listwise religion lastnames\n",
            "0.497 (0.030)\n",
            "0.935 (0.000)\n",
            "0.096 (0.293)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQDx78aOsjy",
        "outputId": "8776528a-cb36-40e7-ff8a-e5bf50963b25"
      },
      "source": [
        "print(\"### RELIGION PCA DEBIASING RESULTS: 300 DIM GLOVE ###\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion entities\")\n",
        "v = d_religion_ent_300\n",
        "run_deb(13,16)\n",
        "\n",
        "print(\"\\nReligion debiased with listwise religion lastnames\")\n",
        "v = d_religion_300\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### RELIGION PCA DEBIASING RESULTS: 300 DIM GLOVE ###\n",
            "\n",
            "Religion debiased with listwise religion entities\n",
            "1.189 (0.000)\n",
            "1.407 (0.000)\n",
            "1.645 (0.000)\n",
            "\n",
            "Religion debiased with listwise religion lastnames\n",
            "0.852 (0.001)\n",
            "0.698 (0.002)\n",
            "1.433 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZCuYopPAhH"
      },
      "source": [
        "### Linear Projection based debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPvqEOWPO36e",
        "outputId": "92e1269f-d3b6-46ab-d75f-acc5d5b2d30d"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Religion Debiasing Linear Projection: 50 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_50[\"आचार्य\"]) - np.array(hindi_glove_50[\"नासिर\"])\n",
        "print(\"\\nReligion debiased with Linear Direction (acharya-nasir):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Religion Debiasing Linear Projection: 50 Dim GloVe\n",
            "\n",
            "Religion debiased with Linear Direction (acharya-nasir):\n",
            "1.067 (0.000)\n",
            "1.614 (0.000)\n",
            "0.694 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y771DuFNPS6d",
        "outputId": "7260d4d9-fd1e-461e-d30e-3beec3ae9402"
      },
      "source": [
        "########## Single direction Linear Debiasing #########\n",
        "\n",
        "print(\"Religion Debiasing Linear Projection: 300 Dim GloVe\")\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = np.array(hindi_glove_300[\"आचार्य\"]) - np.array(hindi_glove_300[\"नासिर\"])\n",
        "print(\"\\nReligion debiased with Linear Direction (acharya-nasir):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Religion Debiasing Linear Projection: 300 Dim GloVe\n",
            "\n",
            "Religion debiased with Linear Direction (acharya-nasir):\n",
            "1.221 (0.000)\n",
            "1.429 (0.000)\n",
            "1.518 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtc6XsItQIh6"
      },
      "source": [
        "### Zhou Based debiasing SEAT results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9FUaZMUP3oo",
        "outputId": "5b26dfda-7b60-4815-a0b9-76c9b338627a"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_50, sample=1000, debiased_weat=True)))\n",
        "\n",
        "v = d_religion_50 - np.dot(d_religion_50, d_religion_ent_50)*d_religion_ent_50\n",
        "print(\"\\nReligion debiased with Zhou (subtracting entities direction):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Religion debiased with Zhou (subtracting entities direction):\n",
            "0.865 (0.000)\n",
            "1.126 (0.000)\n",
            "1.174 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km0Y2aRsQU20",
        "outputId": "3563556a-1cfa-48b6-80be-e8c8c287f394"
      },
      "source": [
        "########## Zhou type #########\n",
        "\n",
        "def run_deb(l,r):\n",
        "  for i in range(l,r,1):\n",
        "    print('{0:.3f} ({1:.3f})'.format(seat_effect_size(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, debiased_weat=True), seat_p_value(X[i][\"list\"], Y[i][\"list\"], X[i][\"type\"], A[i][\"list\"], B[i][\"list\"], A[i][\"type\"], hindi_glove_300, sample=1000, debiased_weat=True)))\n",
        "\n",
        "\n",
        "v = d_religion_300 - np.dot(d_religion_300, d_religion_ent_300)*d_religion_ent_300\n",
        "print(\"\\nReligion debiased with Zhou (subtracting entities direction):\")\n",
        "run_deb(13,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Religion debiased with Zhou (subtracting entities direction):\n",
            "0.852 (0.000)\n",
            "0.684 (0.002)\n",
            "1.497 (0.000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pwg3DrhQljF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}